{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesserae v5 Demo\n",
    "\n",
    "This demo will go over the basics of Tesserae v5 development up through February 5, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x7f4a1e08a340>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from tesserae.db import TessMongoConnection\n",
    "from tesserae.db.entities import Match, Text, Token, Unit\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
    "from tesserae.unitizer import Unitizer\n",
    "from tesserae.matchers.sparse_encoding import SparseMatrixSearch\n",
    "from tesserae.matchers.text_options import TextOptions\n",
    "\n",
    "# Set up the connection and clean up the database\n",
    "connection = TessMongoConnection('127.0.0.1', 27017, None, None, 'tesstest')\n",
    "\n",
    "# Clean up the previous demo\n",
    "connection.connection['features'].delete_many({})\n",
    "connection.connection['matches'].delete_many({})\n",
    "connection.connection['match_sets'].delete_many({})\n",
    "connection.connection['texts'].delete_many({})\n",
    "connection.connection['tokens'].delete_many({})\n",
    "connection.connection['units'].delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Storing New Texts\n",
    "\n",
    "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
    "\n",
    "We start by loading in some metadata from `text_metadata.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title          Author         Language       Year\n",
      "-----          ------         --------       ----\n",
      "aeneid         vergil         latin          19             \n",
      "de oratore     cicero         latin          38             \n",
      "heracles       euripides      greek          -416           \n",
      "epistles       plato          greek          -280           \n"
     ]
    }
   ],
   "source": [
    "with open('text_metadata.json', 'r') as f:\n",
    "    text_meta = json.load(f)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
    "for t in text_meta:\n",
    "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then insert the new texts with `TessMongoConnection.insert` after converting the raw JSON to Tesserae `Text` entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 4 texts.\n",
      "[ObjectId('651c51139c8f571a16438d42'), ObjectId('651c51139c8f571a16438d43'), ObjectId('651c51139c8f571a16438d44'), ObjectId('651c51139c8f571a16438d45')]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for t in text_meta:\n",
    "    texts.append(Text.json_decode(t))\n",
    "result = connection.insert(texts)\n",
    "print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
    "print(result.inserted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the inserted texts with `TessMongoConnection.find`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title          Author         Language       Year\n",
      "aeneid         vergil         latin          19\n",
      "de oratore     cicero         latin          38\n",
      "heracles       euripides      greek          -416\n",
      "epistles       plato          greek          -280\n"
     ]
    }
   ],
   "source": [
    "texts = connection.find('texts', _id=result.inserted_ids)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "for t in texts:\n",
    "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .tess Files\n",
    "\n",
    "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la/vergil.aeneid.tess\n",
      "9908\n",
      "<verg. aen. 1.271>\ttransferet, et longam multa vi muniet Albam.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tessfile = TessFile(texts[0].path, metadata=texts[0])\n",
    "\n",
    "print(tessfile.path)\n",
    "print(len(tessfile))\n",
    "print(tessfile[270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<verg. aen. 1.1>\tArma virumque cano, Troiae qui primus ab oris\n",
      "\n",
      "<verg. aen. 1.2>\tItaliam, fato profugus, Laviniaque venit\n",
      "\n",
      "<verg. aen. 1.3>\tlitora, multum ille et terris iactatus et alto\n",
      "\n",
      "<verg. aen. 1.4>\tvi superum saevae memorem Iunonis ob iram;\n",
      "\n",
      "<verg. aen. 1.5>\tmulta quoque et bello passus, dum conderet urbem,\n",
      "\n",
      "<verg. aen. 1.6>\tinferretque deos Latio, genus unde Latinum,\n",
      "\n",
      "<verg. aen. 1.7>\tAlbanique patres, atque altae moenia Romae.\n",
      "\n",
      "<verg. aen. 1.8>\tMusa, mihi causas memora, quo numine laeso,\n",
      "\n",
      "<verg. aen. 1.9>\tquidve dolens, regina deum tot volvere casus\n",
      "\n",
      "<verg. aen. 1.10>\tinsignem pietate virum, tot adire labores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = tessfile.readlines()\n",
    "for i in range(10):\n",
    "    print(next(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arma\n",
      "virumque\n",
      "cano,\n",
      "Troiae\n",
      "qui\n",
      "primus\n",
      "ab\n",
      "oris\n",
      "Italiam,\n",
      "fato\n"
     ]
    }
   ],
   "source": [
    "tokens = tessfile.read_tokens()\n",
    "for i in range(10):\n",
    "    print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing a Text\n",
    "\n",
    "Texts can be tokenized with `tesserae.tokenizers` objects. These objects are designed to normalize and compute features for tokens of a specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127526 9896 43223\n",
      "Raw            Normalized     Lemmata             Frequency\n",
      "---            ----------     -------             ---------\n",
      "Arma           arma                arma                148\n",
      "virumque       uirumque            uir                 4\n",
      "cano           cano                canus               3\n",
      "Troiae         troiae              troius              36\n",
      "qui            qui                 quis                166\n",
      "primus         primus              primus              34\n",
      "ab             ab                  ab                  136\n",
      "oris           oris                ora                 32\n",
      "Italiam        italiam             italia              32\n",
      "fato           fato                for                 7\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
    "\n",
    "tokens, tags, features = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
    "\n",
    "print(len(tokens), len(tags), len(features))\n",
    "\n",
    "print('{}{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'.ljust(20), 'Frequency'))\n",
    "print('{}{}{}{}'.format('---'.ljust(15), '----------'.ljust(15), '-------'.ljust(20), '---------'))\n",
    "for i in range(20):\n",
    "    if len(tokens[i].features):\n",
    "        print('{}{}{}{}'.format(tokens[i].display.ljust(15),\n",
    "                              str(tokens[i].features['form'].token).ljust(20),\n",
    "                              str(tokens[i].features['lemmata'][0].token).ljust(20),\n",
    "                              list(tokens[i].features['form'].frequencies.values())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed tokens can then be stored in and retrieved from the database, similar to text metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 43223 feature entities out of 43223\n"
     ]
    }
   ],
   "source": [
    "result = connection.insert(features)\n",
    "print('Inserted {} feature entities out of {}'.format(len(result.inserted_ids), len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitizing a Text\n",
    "\n",
    "Texts can be unitized into lines and phrases, and the intertext matches are found between units of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines\n",
      "-----\n",
      "['1.1']: Arma virumque cano Troiae qui primus ab oris \n",
      "['1.2']: Italiam fato profugus Laviniaque venit \n",
      "['1.3']: litora multum ille et terris iactatus et alto \n",
      "['1.4']: vi superum saevae memorem Iunonis ob iram \n",
      "['1.5']: multa quoque et bello passus dum conderet urbem \n",
      "['1.6']: inferretque deos Latio genus unde Latinum \n",
      "['1.7']: Albanique patres atque altae moenia Romae \n",
      "['1.8']: Musa mihi causas memora quo numine laeso \n",
      "['1.9']: quidve dolens regina deum tot volvere casus \n",
      "['1.10']: insignem pietate virum tot adire labores \n",
      "['1.11']: impulerit Tantaene animis caelestibus irae \n",
      "['1.12']: Urbs antiqua fuit Tyrii tenuere coloni \n",
      "['1.13']: Karthago Italiam contra Tiberinaque longe \n",
      "['1.14']: ostia dives opum studiisque asperrima belli \n",
      "['1.15']: quam Iuno fertur terris magis omnibus unam \n",
      "['1.16']: posthabita coluisse Samo hic illius arma \n",
      "['1.17']: hic currus fuit hoc regnum dea gentibus esse \n",
      "['1.18']: si qua fata sinant iam tum tenditque fovetque \n",
      "['1.19']: Progeniem sed enim Troiano a sanguine duci \n",
      "['1.20']: audierat Tyrias olim quae verteret arces \n",
      "\n",
      "\n",
      "Phrases\n",
      "-------\n",
      "['1.1', '1.2', '1.3', '1.4']: Arma virumque cano Troiae qui primus ab oris Italiam fato profugus Laviniaque venit litora multum ille et terris iactatus et alto vi superum saevae memorem Iunonis ob iram \n",
      "['1.5', '1.6', '1.7']: multa quoque et bello passus dum conderet urbem inferretque deos Latio genus unde Latinum Albanique patres atque altae moenia Romae \n",
      "['1.8', '1.9', '1.10', '1.11']: Musa mihi causas memora quo numine laeso quidve dolens regina deum tot volvere casus insignem pietate virum tot adire labores impulerit \n",
      "['1.11']: Tantaene animis caelestibus irae \n",
      "['1.12', '1.13', '1.14']: Urbs antiqua fuit Tyrii tenuere coloni Karthago Italiam contra Tiberinaque longe ostia dives opum studiisque asperrima belli \n",
      "['1.15', '1.16']: quam Iuno fertur terris magis omnibus unam posthabita coluisse Samo \n",
      "['1.16', '1.17']: hic illius arma hic currus fuit \n",
      "['1.17', '1.18']: hoc regnum dea gentibus esse si qua fata sinant iam tum tenditque fovetque \n",
      "['1.19', '1.20']: Progeniem sed enim Troiano a sanguine duci audierat Tyrias olim quae verteret arces \n",
      "['1.21', '1.22']: hinc populum late regem belloque superbum venturum excidio Libyae \n",
      "['1.22']: sic volvere Parcas \n",
      "['1.23', '1.24', '1.25', '1.26']: Id metuens veterisque memor Saturnia belli prima quod ad Troiam pro caris gesserat Argis necdum etiam causae irarum saevique dolores exciderant animo \n",
      "['1.26', '1.27', '1.28']: manet alta mente repostum iudicium Paridis spretaeque iniuria formae et genus invisum et rapti Ganymedis honores \n",
      "['1.29', '1.30', '1.31', '1.32']: His accensa super iactatos aequore toto Troas reliquias Danaum atque immitis Achilli arcebat longe Latio multosque per annos errabant acti fatis maria omnia circum \n",
      "['1.33']: Tantae molis erat Romanam condere gentem \n",
      "['1.34', '1.35', '1.36', '1.37']: Vix e conspectu Siculae telluris in altum vela dabant laeti et spumas salis aere ruebant cum Iuno aeternum servans sub pectore volnus haec secum \n",
      "['1.37', '1.38']: Mene incepto desistere victam nec posse Italia Teucrorum avertere regem \n",
      "['1.39']: Quippe vetor fatis \n",
      "['1.39', '1.40', '1.41']: Pallasne exurere classem Argivum atque ipsos potuit submergere ponto unius ob noxam et furias Aiacis Oilei \n",
      "['1.42', '1.43', '1.44', '1.45']: Ipsa Iovis rapidum iaculata e nubibus ignem disiecitque rates evertitque aequora ventis illum expirantem transfixo pectore flammas turbine corripuit scopuloque infixit acuto \n"
     ]
    }
   ],
   "source": [
    "# Unitizing lines of a poem\n",
    "unitizer = Unitizer()\n",
    "lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
    "\n",
    "print('Lines\\n-----')\n",
    "for line in lines[:20]:\n",
    "        print(''.join([str(line.tags), ': '] + [t['display']+' ' for t in line.tokens]))\n",
    "        \n",
    "print('\\n\\nPhrases\\n-------')\n",
    "for phrase in phrases[:20]:\n",
    "        print(''.join([str(phrase.tags), ': '] + [t['display']+' ' for t in phrase.tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 14983 units out of 14983.\n",
      "Inserted 127526 tokens out of 127526.\n"
     ]
    }
   ],
   "source": [
    "# Unitizing phrases of a poem or prose\n",
    "result = connection.insert(lines + phrases)\n",
    "print('Inserted {} units out of {}.'.format(len(result.inserted_ids), len(lines + phrases)))\n",
    "\n",
    "\n",
    "result = connection.insert(tokens)\n",
    "print('Inserted {} tokens out of {}.'.format(len(result.inserted_ids), len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts[2:]:\n",
    "    tessfile = TessFile(text.path, metadata=text)\n",
    "    tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
    "    \n",
    "    tokens, tags, features = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
    "        \n",
    "    result = connection.insert(features)\n",
    "\n",
    "    unitizer = Unitizer()\n",
    "    lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
    "    result = connection.insert(lines + phrases)\n",
    "    \n",
    "    result = connection.insert(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "Once the Texts, Tokens, and Units are in the database, we can then find intertext matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "match() missing 1 required positional argument: 'feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m match_texts \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreek\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m matches, match_set \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTextOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphrase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTextOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphrase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_basis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted matching in \u001b[39m\u001b[38;5;132;01m{0:.2f}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start))\n\u001b[1;32m     10\u001b[0m matches\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mscore, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: match() missing 1 required positional argument: 'feature'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "matcher = SparseMatrixSearch(connection)\n",
    "match_texts = [t for t in texts if t.language == 'greek']\n",
    "\n",
    "start = time.time()\n",
    "matches, match_set = matcher.match(TextOptions(match_texts[0],'phrase'),TextOptions(match_texts[1],'phrase'), \n",
    "                                   'form', distance_basis='span', stopwords=20, max_distance=10)\n",
    "print(\"Completed matching in {0:.2f}s\".format(time.time() - start))\n",
    "\n",
    "matches.sort(key=lambda x: x.score, reverse=True)\n",
    "\n",
    "# result = connection.insert(match_set)\n",
    "# print('Inserted {} match set entities out of {}'.format(len(result.inserted_ids), 1))\n",
    "result = connection.insert(matches)\n",
    "print('Inserted {} match entities out of {}'.format(len(result.inserted_ids), len(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "matches = connection.aggregate('matches', [\n",
    "    {'$match': {'match_set': match_set.id}},\n",
    "    {'$sort': {'score': -1}},\n",
    "    {'$limit': 20},\n",
    "    {'$lookup': {\n",
    "        'from': 'units',\n",
    "        'let': {'m_units': '$units'},\n",
    "        'pipeline': [\n",
    "            {'$match': {'$expr': {'$in': ['$_id', '$$m_units']}}},\n",
    "            {'$lookup': {\n",
    "                'from': 'tokens',\n",
    "                'localField': '_id',\n",
    "                'foreignField': 'phrase',\n",
    "                'as': 'tokens'\n",
    "            }},\n",
    "            {'$sort': {'index': 1}}\n",
    "        ],\n",
    "        'as': 'units'\n",
    "    }},\n",
    "    {'$lookup': {\n",
    "        'from': 'tokens',\n",
    "        'localField': 'tokens',\n",
    "        'foreignField': '_id',\n",
    "        'as': 'tokens'\n",
    "    }},\n",
    "    {'$project': {\n",
    "        'units': True,\n",
    "        'score': True,\n",
    "        'tokens': '$tokens.feature_set'\n",
    "    }},\n",
    "    {'$lookup': {\n",
    "        'from': 'feature_sets',\n",
    "        'localField': 'tokens',\n",
    "        'foreignField': '_id',\n",
    "        'as': 'tokens'\n",
    "    }}\n",
    "])\n",
    "\n",
    "print('\\n')\n",
    "print('{}{}'.format('Score'.ljust(15), 'Match Tokens'.ljust(15)))\n",
    "print('{}{}'.format('-----'.ljust(15), '------------'.ljust(15)))\n",
    "for m in matches:\n",
    "    print('{}{}'.format(('%.3f'%(m.score)).ljust(15), ', '.join(list(set([t['form'] for t in m.tokens])))))\n",
    "    print('{} {} {}: {}'.format(match_texts[0].author, match_texts[0].title, m.units[0]['tags'], ''.join([t['display'] for t in m.units[0]['tokens']])))\n",
    "    print('{} {} {}: {}'.format(match_texts[1].author, match_texts[1].title, m.units[1]['tags'], ''.join([t['display'] for t in m.units[1]['tokens']])))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
